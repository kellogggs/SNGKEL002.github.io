{
  "hash": "8ca79d87adf4f06e19f1e6d3e37d62cc",
  "result": {
    "markdown": "---\ntitle: \"a1\"\nauthor: \"Kelly-Robyn Singh\"\ndate: \"2023-10-25\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nFigure 1: Barplot Showing the Number of Speeches Given Per Year.\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nFigure 2: Barplot Showing the Number of Speeches Given By Each President.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\nFigure 3: Barplot Showing the Speech Length of Each Speech.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nFigure 4: Plot Showing the Accuracy and Loss of The Feed Forward Neural Network Model Using Bag of Words Features .\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nFigure 5: Plot Showing the Accuracy and Loss of The Feed Forward Neural Network Model Using TF-IDF Features .\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nFigure 7: Plot Showing the Accuracy and Loss of The Feed Forward Neural Network Model Using Word Embeddings.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\nFigure 8: Plot Showing the Accuracy and Loss of The Convolutional Neural Network Model Using Word Embeddings.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n226/226 - 27s - loss: 1.2546 - accuracy: 0.4361 - val_loss: 1.5732 - val_accuracy: 0.0000e+00 - 27s/epoch - 119ms/step\nEpoch 2/10\n226/226 - 15s - loss: 1.0733 - accuracy: 0.5197 - val_loss: 1.6015 - val_accuracy: 0.0000e+00 - 15s/epoch - 68ms/step\nEpoch 3/10\n226/226 - 15s - loss: 0.9592 - accuracy: 0.5795 - val_loss: 1.6730 - val_accuracy: 0.2452 - 15s/epoch - 66ms/step\nEpoch 4/10\n226/226 - 15s - loss: 0.8459 - accuracy: 0.6457 - val_loss: 1.3682 - val_accuracy: 0.3790 - 15s/epoch - 68ms/step\nEpoch 5/10\n226/226 - 14s - loss: 0.7807 - accuracy: 0.6864 - val_loss: 1.5090 - val_accuracy: 0.3599 - 14s/epoch - 63ms/step\nEpoch 6/10\n226/226 - 14s - loss: 0.7171 - accuracy: 0.7112 - val_loss: 1.3820 - val_accuracy: 0.4283 - 14s/epoch - 63ms/step\nEpoch 7/10\n226/226 - 14s - loss: 0.6764 - accuracy: 0.7316 - val_loss: 1.5129 - val_accuracy: 0.4220 - 14s/epoch - 63ms/step\nEpoch 8/10\n226/226 - 14s - loss: 0.6395 - accuracy: 0.7505 - val_loss: 1.4325 - val_accuracy: 0.4586 - 14s/epoch - 64ms/step\nEpoch 9/10\n226/226 - 15s - loss: 0.5897 - accuracy: 0.7762 - val_loss: 1.4902 - val_accuracy: 0.4363 - 15s/epoch - 66ms/step\nEpoch 10/10\n226/226 - 16s - loss: 0.5519 - accuracy: 0.7836 - val_loss: 1.7050 - val_accuracy: 0.4618 - 16s/epoch - 71ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n110/110 - 3s - loss: 1.2865 - accuracy: 0.5513 - 3s/epoch - 23ms/step\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nFigure 9: Plot Showing the Accuracy and Loss of The Recurrent Neural Network Model Using Word Embeddings.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Table 1: Neural Network Metrics</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Neural.Network.Type </th>\n   <th style=\"text-align:left;\"> Data.Type </th>\n   <th style=\"text-align:right;\"> Loss </th>\n   <th style=\"text-align:right;\"> Accuracy </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Multilayer Perceptron </td>\n   <td style=\"text-align:left;\"> Bag of Words </td>\n   <td style=\"text-align:right;\"> 2.01 </td>\n   <td style=\"text-align:right;\"> 0.55 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Multilayer Perceptron </td>\n   <td style=\"text-align:left;\"> TFIDF </td>\n   <td style=\"text-align:right;\"> 2.78 </td>\n   <td style=\"text-align:right;\"> 0.51 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Multilayer Perceptron </td>\n   <td style=\"text-align:left;\"> Word Embeddings </td>\n   <td style=\"text-align:right;\"> 2.74 </td>\n   <td style=\"text-align:right;\"> 0.47 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Convolutional Neural Networks </td>\n   <td style=\"text-align:left;\"> Word Embeddings </td>\n   <td style=\"text-align:right;\"> 1.85 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Recurrent Neural Network </td>\n   <td style=\"text-align:left;\"> Word Embeddings </td>\n   <td style=\"text-align:right;\"> 1.40 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n:::\n\nTable 1: Table Showing Neural Network Metrics\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](SNGKEL002_A1_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\nFigure 6: Histogram Showing the Sequence Length After Tokenization",
    "supporting": [
      "SNGKEL002_A1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}